{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BKETw_9QdjDA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "import sparknlp\n",
    "from pyspark.sql.functions import col, regexp_extract, expr\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp import MultiDocumentAssembler, Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ez888ixNdk3s"
   },
   "outputs": [],
   "source": [
    "# create spark session, fetch required packages\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"locaSearchEngine\") \\\n",
    "    .config(\"spark.jars.packages\", ','.join(\n",
    "        [\n",
    "            'com.johnsnowlabs.nlp:spark-nlp_2.12:4.2.0',\n",
    "            'com.databricks:spark-xml_2.12:0.15.0',\n",
    "            'org.elasticsearch:elasticsearch-spark-30_2.12:8.4.2'\n",
    "        ])) \\\n",
    "    .config(\"sparkdriver.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxs8ah50doUb"
   },
   "outputs": [],
   "source": [
    "# read the dump file of the simplewiki (XML format). mediawiki is the root level tag and page is the row tag for each article\n",
    "df = spark.read\\\n",
    "    .format('xml')\\\n",
    "    .option(\"rowTag\", \"page\")\\\n",
    "    .option(\"rootTag\", \"mediawiki\")\\\n",
    "    .load(\"path_to_simplewiki_xml\")\\\n",
    "    .repartition(100)\\\n",
    "    .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7iB0rX7dp0L"
   },
   "outputs": [],
   "source": [
    "# make sure text field exists and that the page is not a redirect and keep only the title and text\n",
    "df = df.filter('redirect IS NULL').selectExpr(\n",
    "    'revision.text._VALUE AS text',\n",
    "    'title'\n",
    ").filter('text IS NOT NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract categories from text using regex and add it to the dataframe\n",
    "df = df.withColumn(\"category\", expr(r\"array_join(regexp_extract_all(text, 'Category:([^]|]+)', 1), ' ')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "wp74hJYwds8N",
    "outputId": "a7589b14-4bd3-4478-eee6-a92dac88acce"
   },
   "outputs": [],
   "source": [
    "\n",
    "# process text into data format that is processabel by Spark NLP\n",
    "document_assembler = MultiDocumentAssembler()\\\n",
    "    .setInputCols(['text', 'category'])\\\n",
    "    .setOutputCols(['document1', 'document2'])\n",
    "# tokenize the input text\n",
    "tokenizer1 = Tokenizer()\\\n",
    "    .setInputCols(['document1'])\\\n",
    "    .setOutputCol('tokens1')\n",
    "tokenizer2 = Tokenizer()\\\n",
    "    .setInputCols(['document2'])\\\n",
    "    .setOutputCol('tokens2')\n",
    "# create lemmas for the tokens\n",
    "lemmatizer_model1 = LemmatizerModel.pretrained()\\\n",
    "    .setInputCols(['tokens1'])\\\n",
    "    .setOutputCol('lemmas1')\n",
    "lemmatizer_model2 = LemmatizerModel.pretrained()\\\n",
    "    .setInputCols(['tokens2'])\\\n",
    "    .setOutputCol('lemmas2')\n",
    "# remove punctuation from the lemmas\n",
    "# turn lemmas to lower case and remove unwanted symbols\n",
    "normalizer1 = Normalizer()\\\n",
    "    .setCleanupPatterns([\n",
    "        '[^\\w\\d\\s]'\n",
    "    ])\\\n",
    "    .setInputCols(['lemmas1'])\\\n",
    "    .setOutputCol('normalized1')\\\n",
    "    .setLowercase(True)\n",
    "normalizer2 = Normalizer()\\\n",
    "    .setCleanupPatterns([\n",
    "        '[^\\w\\d\\s]'\n",
    "    ])\\\n",
    "    .setInputCols(['lemmas2'])\\\n",
    "    .setOutputCol('normalized2')\\\n",
    "    .setLowercase(True)\n",
    "# convert result of annotators into strings\n",
    "finisher = Finisher()\\\n",
    "    .setInputCols(['normalized1', 'normalized2'])\\\n",
    "    .setOutputCols(['normalized1', 'normalized2'])\n",
    "\n",
    "# fit nlp pipeline to the data and set stages execution order\n",
    "nlp_pipeline = Pipeline()\\\n",
    "    .setStages([document_assembler, tokenizer1, tokenizer2, lemmatizer_model1, lemmatizer_model2, normalizer1, normalizer2, finisher])\\\n",
    "    .fit(df)\n",
    "\n",
    "# selects the columns from df and returns a new df when transforming the data using the nlp pipeline\n",
    "cleaned_df = nlp_pipeline.transform(df).selectExpr(\n",
    "    'text',\n",
    "    'title',\n",
    "    'category',\n",
    "    'array_join(normalized1, \" \") AS normalized_text',\n",
    "    'array_join(normalized2, \" \") AS normalized_category'\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6C4WutzWd_jk"
   },
   "outputs": [],
   "source": [
    "# local instance of elastic, credentials need to be changed if running on other machine\n",
    "cleaned_df.write\\\n",
    "    .format('org.elasticsearch.spark.sql')\\\n",
    "    .option('es.nodes', 'localhost')\\\n",
    "    .option('es.port', '9200')\\\n",
    "    .option('es.nodes.wan.only', 'true')\\\n",
    "    .option(\"es.net.http.auth.user\", \"elastic\")\\\n",
    "    .option(\"es.net.http.auth.pass\", \"key\") \\\n",
    "    .mode(\"Overwrite\")\\\n",
    "    .save('simpleenglish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ksR1iB0hoKu"
   },
   "outputs": [],
   "source": [
    "# score is set to value the relative importance of the text, category and title for the search result\n",
    "\n",
    "def query_index(query, fields=[('normalized_text', 1), ('normalized_category', 5), ('title', 10)], size=10):\n",
    "    dataframe = spark.createDataFrame([(query, '')], ('text', 'category'))\n",
    "    cleaned_row = nlp_pipeline.transform(dataframe).first()\n",
    "    query = cleaned_row['text']\n",
    "\n",
    "    data = {\n",
    "        \"_source\": ['title', 'normalized_category'],\n",
    "        \"query\": { \n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        # combine score from all fields\n",
    "                        \"type\": \"most_fields\",\n",
    "                        # apply weights to fields\n",
    "                        \"fields\": ['{}^{}'.format(f, b) for f, b in fields],\n",
    "                        # all terms must be present\n",
    "                        \"operator\": \"and\"\n",
    "                    }\n",
    "                },\n",
    "                # boost score if input text matches title exactly\n",
    "                \"should\": [{\n",
    "                    \"match_phrase\": {\n",
    "                        \"title\": {\n",
    "                            \"query\": query,\n",
    "                            \"boost\": 30\n",
    "                        }\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    params = (\n",
    "        ('pretty', ''), ('size', size)\n",
    "    )\n",
    "\n",
    "    response = requests.post(\n",
    "        'http://elastic:instance@localhost:9200/simpleenglish/_search', \n",
    "        headers=headers, params=params, \n",
    "        data=json.dumps(data)).json()\n",
    "    \n",
    "    return [(r['_source']['title'], r['_source']['normalized_category'], r['_score']) for r in response['hits']['hits']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run query and get search result\n",
    "query_string = 'spicy food'\n",
    "query_results = query_index(query_string)\n",
    "query_results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO9Cge+OaPAkv080OytB/WD",
   "include_colab_link": true,
   "name": "3.14_Search Engine.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
